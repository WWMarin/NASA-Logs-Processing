{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing NASA Logs  \n",
    "  \n",
    "This challenge was solved using Spark SQL to clean and wrangle the data to answer the proposed questions.  \n",
    "  \n",
    "The challenge consists of answering the following questions:  \n",
    "  \n",
    "- Number of unique hosts  \n",
    "- Total number of 404 errors  \n",
    "- The 5 URLs that caused the most 404 errors  \n",
    "- Number of daily 404 errors  \n",
    "- Total bytes returned  \n",
    "  \n",
    "Official dataset source:  \n",
    "https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html  \n",
    "  \n",
    "Data:  \n",
    "ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz  \n",
    "ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz  \n",
    "\n",
    "About the dataset:  \n",
    "The dataset holds all the HTTP requests to the server at NASA'S Kennedy Space Center for July and August of 1995.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Processing NASA Logs').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading files into a dataframe\n",
    "df = spark.read.csv('NASA_access_log_*.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting each row into columns\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "split_col = split(df['_c0'], ' ')\n",
    "df = df.withColumn('host', split_col.getItem(0)) \\\n",
    "       .withColumn('rfc931', split_col.getItem(1)) \\\n",
    "       .withColumn('username', split_col.getItem(2)) \\\n",
    "       .withColumn('date:time', split_col.getItem(3)) \\\n",
    "       .withColumn('timezone', split_col.getItem(4)) \\\n",
    "       .withColumn('method', split_col.getItem(5)) \\\n",
    "       .withColumn('resource', split_col.getItem(6)) \\\n",
    "       .withColumn('protocol', split_col.getItem(7)) \\\n",
    "       .withColumn('statuscode', split_col.getItem(8)) \\\n",
    "       .withColumn('bytes', split_col.getItem(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging appropriate columns\n",
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "df = df.withColumn('date_time_timezone', concat(col('date:time'), lit(' '), col('timezone')))\n",
    "df = df.withColumn('request', concat(col('method'), lit(' '), col('resource'), lit(' '), col('protocol')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping redundant columns\n",
    "df = df.drop('_c0', 'date:time', 'timezone', 'method', 'resource', 'protocol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering the dataframe\n",
    "df = df.select('host', 'rfc931', 'username', 'date_time_timezone', 'request', 'statuscode', 'bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning request and date_time_timezone columns\n",
    "from pyspark.sql.functions import expr, substring\n",
    "\n",
    "df = df.withColumn('request', expr('substring(request, 2, length(request)-2)'))\n",
    "df = df.withColumn('date_time_timezone', substring(col('date_time_timezone'), 2, 26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting date_time_timezone as TimeStamp and bytes as Int\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.withColumn('date_time_timezone', to_timestamp(col('date_time_timezone'), 'dd/MMM/yyyy:HH:mm:ss Z'))\n",
    "df = df.withColumn('bytes', col('bytes').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating view with the dataframe\n",
    "df.createOrReplaceTempView('accessLog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+-------------------+--------------------+----------+-----+\n",
      "|                host|rfc931|username| date_time_timezone|             request|statuscode|bytes|\n",
      "+--------------------+------+--------+-------------------+--------------------+----------+-----+\n",
      "|        199.72.81.55|     -|       -|1995-07-01 01:00:01|GET /history/apol...|       200| 6245|\n",
      "|unicomp6.unicomp.net|     -|       -|1995-07-01 01:00:06|GET /shuttle/coun...|       200| 3985|\n",
      "|      199.120.110.21|     -|       -|1995-07-01 01:00:09|GET /shuttle/miss...|       200| 4085|\n",
      "|  burger.letters.com|     -|       -|1995-07-01 01:00:11|GET /shuttle/coun...|       304|    0|\n",
      "|      199.120.110.21|     -|       -|1995-07-01 01:00:11|GET /shuttle/miss...|       200| 4179|\n",
      "|  burger.letters.com|     -|       -|1995-07-01 01:00:12|GET /images/NASA-...|       304|    0|\n",
      "|  burger.letters.com|     -|       -|1995-07-01 01:00:12|GET /shuttle/coun...|       200|    0|\n",
      "|     205.212.115.106|     -|       -|1995-07-01 01:00:12|GET /shuttle/coun...|       200| 3985|\n",
      "|         d104.aa.net|     -|       -|1995-07-01 01:00:13|GET /shuttle/coun...|       200| 3985|\n",
      "|      129.94.144.152|     -|       -|1995-07-01 01:00:13|      GET / HTTP/1.0|       200| 7074|\n",
      "|unicomp6.unicomp.net|     -|       -|1995-07-01 01:00:14|GET /shuttle/coun...|       200|40310|\n",
      "|unicomp6.unicomp.net|     -|       -|1995-07-01 01:00:14|GET /images/NASA-...|       200|  786|\n",
      "|unicomp6.unicomp.net|     -|       -|1995-07-01 01:00:14|GET /images/KSC-l...|       200| 1204|\n",
      "|         d104.aa.net|     -|       -|1995-07-01 01:00:15|GET /shuttle/coun...|       200|40310|\n",
      "|         d104.aa.net|     -|       -|1995-07-01 01:00:15|GET /images/NASA-...|       200|  786|\n",
      "|         d104.aa.net|     -|       -|1995-07-01 01:00:15|GET /images/KSC-l...|       200| 1204|\n",
      "|      129.94.144.152|     -|       -|1995-07-01 01:00:17|GET /images/ksclo...|       304|    0|\n",
      "|      199.120.110.21|     -|       -|1995-07-01 01:00:17|GET /images/launc...|       200| 1713|\n",
      "|ppptky391.asahi-n...|     -|       -|1995-07-01 01:00:18|GET /facts/about_...|       200| 3977|\n",
      "|  net-1-141.eden.com|     -|       -|1995-07-01 01:00:19|GET /shuttle/miss...|       200|34029|\n",
      "+--------------------+------+--------+-------------------+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- rfc931: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- date_time_timezone: timestamp (nullable = true)\n",
      " |-- request: string (nullable = true)\n",
      " |-- statuscode: string (nullable = true)\n",
      " |-- bytes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizing the dataframe\n",
    "df.show() \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of unique hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the number of unique hosts\n",
    "unHosts = spark.sql('SELECT COUNT(DISTINCT host) \\\n",
    "                     AS Unique_Hosts \\\n",
    "                     FROM accessLog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Unique_Hosts|\n",
      "+------------+\n",
      "|      137979|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizing the dataframe\n",
    "unHosts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of 404 errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the total of requests that returned a 404 error\n",
    "totalErrors = spark.sql('SELECT COUNT(statuscode) \\\n",
    "                         AS 404_Errors \\\n",
    "                         FROM accessLog \\\n",
    "                         WHERE statuscode == 404')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|404_Errors|\n",
      "+----------+\n",
      "|     20638|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizing the dataframe\n",
    "totalErrors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 5 URLs that caused the most 404 errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the five URLs that returned the most 404 errors\n",
    "totalErrors = spark.sql('SELECT host, COUNT(statuscode) \\\n",
    "                         AS 404_Errors \\\n",
    "                         FROM accessLog \\\n",
    "                         WHERE statuscode == 404 \\\n",
    "                         GROUP BY host \\\n",
    "                         ORDER BY COUNT(statuscode) DESC \\\n",
    "                         LIMIT 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                host|404_Errors|\n",
      "+--------------------+----------+\n",
      "|hoohoo.ncsa.uiuc.edu|       251|\n",
      "|piweba3y.prodigy.com|       157|\n",
      "|jbiagioni.npt.nuw...|       132|\n",
      "|piweba1y.prodigy.com|       114|\n",
      "|www-d4.proxy.aol.com|        91|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizing the dataframe\n",
    "totalErrors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of daily 404 errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the number of 404 errors for each day\n",
    "dailyErrors = spark.sql('SELECT date_time_timezone, COUNT(statuscode) \\\n",
    "                         AS 404_Errors \\\n",
    "                         FROM accessLog \\\n",
    "                         WHERE statuscode == 404 \\\n",
    "                         GROUP BY date_time_timezone \\\n",
    "                         ORDER BY COUNT(statuscode) DESC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "| date_time_timezone|404_Errors|\n",
      "+-------------------+----------+\n",
      "|1995-08-11 13:05:59|         7|\n",
      "|1995-08-28 12:56:35|         7|\n",
      "|1995-08-11 13:05:58|         5|\n",
      "|1995-08-17 17:55:00|         5|\n",
      "|1995-07-12 11:20:43|         5|\n",
      "|1995-08-28 18:14:32|         5|\n",
      "|1995-07-12 11:24:50|         5|\n",
      "|1995-07-12 11:35:12|         5|\n",
      "|1995-07-12 11:35:09|         5|\n",
      "|1995-07-12 11:21:30|         5|\n",
      "|1995-08-28 18:14:42|         5|\n",
      "|1995-07-12 11:35:11|         5|\n",
      "|1995-07-11 15:08:06|         5|\n",
      "|1995-07-12 11:35:01|         4|\n",
      "|1995-07-20 08:21:17|         4|\n",
      "|1995-08-13 15:28:59|         4|\n",
      "|1995-08-28 02:05:47|         4|\n",
      "|1995-08-28 18:14:15|         4|\n",
      "|1995-08-28 18:14:28|         4|\n",
      "|1995-07-06 11:31:11|         4|\n",
      "+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizing the dataframe\n",
    "dailyErrors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total bytes returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the total amount of bytes from all requests\n",
    "totalBytes = spark.sql('SELECT SUM(bytes) \\\n",
    "                        AS Bytes \\\n",
    "                        FROM accessLog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      Bytes|\n",
      "+-----------+\n",
      "|65136540452|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizing the dataframe\n",
    "totalBytes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
